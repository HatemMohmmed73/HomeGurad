# -*- coding: utf-8 -*-
"""Training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BUTZgEKhyyIlUDLv7_VirCSpE93JqmDO

# Training
"""

import numpy as np
import pandas as pd
from sklearn.base import BaseEstimator, TransformerMixin
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.decomposition import PCA
from sklearn.ensemble import IsolationForest
from sklearn.metrics import (
    precision_recall_curve,
    confusion_matrix,
    classification_report,
    f1_score,
    mean_squared_error
)
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

tf.random.set_seed(42)
np.random.seed(42)

DATA_VERSION = "2files"

if DATA_VERSION == "2files":
    TRAIN_PATH = "/content/drive/MyDrive/HOMEGUARD/01_data/train_fold11.parquet"
    VALID_PATH = "/content/drive/MyDrive/HOMEGUARD/01_data/valid_fold11.parquet"
    TEST_PATH  = "/content/drive/MyDrive/HOMEGUARD/02_notebooks/sample_1M.parquet"
    OurData_PATH = "/content/drive/MyDrive/HOMEGUARD/01_data/logs/02_notebooks/check.parquet"



train_df = pd.read_parquet(TRAIN_PATH)
valid_df = pd.read_parquet(VALID_PATH) if VALID_PATH else None
test_df = pd.read_parquet(TEST_PATH)
full_train_df =pd.concat([train_df, valid_df], axis=0, ignore_index=True)

ourdata = pd.read_parquet(OurData_PATH)



class ZeekFixedWindowAggregator(BaseEstimator, TransformerMixin):
    def __init__(self, window_size=60, rolling_window=3):
        self.window_size = window_size
        self.rolling_window = rolling_window

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        X_feat, _ = self._process_pipeline_internal(X)
        return X_feat.values

    def transform_with_metadata(self, X):
        return self._process_pipeline_internal(X)

    # ------------------------------------------------------------------
    # FIXED BIN ASSIGNMENT (Fast & Simple)
    # ------------------------------------------------------------------
    def _assign_fixed_bins(self, df):
        df = df.copy()
        # Each flow belongs to exactly one bin based on start time
        df["time_bin"] = (df["ts"] // self.window_size).astype(int)
        return df

    def _process_pipeline_internal(self, df):
            # 1. Numeric cleanup
            df = df.copy()
            num_cols = ["ts", "orig_bytes", "resp_bytes", "orig_pkts", "resp_pkts", "duration", "missed_bytes"]
            for c in num_cols:
                if c in df.columns:
                    df[c] = pd.to_numeric(df[c], errors="coerce").fillna(0)

            # 2. Assign Bins
            df = self._assign_fixed_bins(df)

            # 3. Determine Grouping
            group_cols = ["id.orig_h", "time_bin"]
            if "capture_id" in df.columns:
                group_cols = ["capture_id", "id.orig_h", "time_bin"]

            # 4. Core Aggregation (Expanded with Mean, Std, and Max)
            # This gives the model a much better "shape" of the traffic
            agg = df.groupby(group_cols).agg(
                # Bytes statistics
                orig_bytes_sum=("orig_bytes", "sum"),
                orig_bytes_mean=("orig_bytes", "mean"),
                orig_bytes_std=("orig_bytes", "std"),
                resp_bytes_sum=("resp_bytes", "sum"),
                resp_bytes_mean=("resp_bytes", "mean"),

                # Packet statistics
                orig_pkts_sum=("orig_pkts", "sum"),
                orig_pkts_mean=("orig_pkts", "mean"),
                resp_pkts_sum=("resp_pkts", "sum"),
                resp_pkts_mean=("resp_pkts", "mean"),

                # Temporal / Flow statistics
                duration_mean=("duration", "mean"),
                duration_std=("duration", "std"),
                flow_count=("ts", "count"),

                # Diversity and Security metrics
                orig_port_diversity=("id.orig_p", "nunique"),
                resp_port_diversity=("id.resp_p", "nunique"),
                missed_bytes_sum=("missed_bytes", "sum"),

                # State ratios
                tcp_ratio=("proto", lambda x: (x == "tcp").mean() if "proto" in df.columns else 0),
                s0_ratio=("conn_state", lambda x: (x == "S0").mean() if "conn_state" in df.columns else 0)
            ).reset_index().fillna(0)

            # 5. Advanced Feature Engineering
            agg["bytes_ratio"] = agg["orig_bytes_sum"] / (agg["resp_bytes_sum"] + 1)
            agg["pkts_per_sec"] = (agg["orig_pkts_sum"] + agg["resp_pkts_sum"]) / self.window_size

            # New: Coefficient of Variation (helps detect "heartbeat" bots)
            # If std is 0, the ratio will be 0
            agg["duration_cv"] = agg["duration_std"] / (agg["duration_mean"] + 1)

            # 6. Log scaling heavy-tailed features
            log_cols = [
                "orig_bytes_sum", "orig_bytes_mean", "resp_bytes_sum",
                "orig_pkts_sum", "resp_pkts_sum", "flow_count"
            ]
            for c in log_cols:
                if c in agg.columns:
                    agg[c] = np.log1p(agg[c])

            # 7. Temporal Smoothing
            if self.rolling_window > 1:
                sort_cols = ["id.orig_h", "time_bin"]
                group_smooth = ["id.orig_h"]
                if "capture_id" in agg.columns:
                    sort_cols = ["capture_id", "id.orig_h", "time_bin"]
                    group_smooth = ["capture_id", "id.orig_h"]

                agg = agg.sort_values(sort_cols)
                feat_cols = agg.select_dtypes(include=[np.number]).columns.difference(["time_bin"])
                agg[feat_cols] = agg.groupby(group_smooth)[feat_cols].transform(
                    lambda x: x.rolling(window=self.rolling_window, min_periods=1).mean()
                )

            # 8. Final Formatting
            out = agg.fillna(0)
            X_feat = out.drop(columns=["time_bin", "id.orig_h", "capture_id"], errors="ignore")

            return X_feat, out

    def _generate_labels_internal(self, raw_df, behavior_df, drop_mixed=True):
            """
            Generates binary labels for fixed windows.
            If drop_mixed=True, windows containing both malicious and benign
            flows are removed to ensure 'pure' training data.
            """
            # 1. Assign Fixed Bins (matching the aggregator logic)
            df = self._assign_fixed_bins(raw_df)

            # 2. Binary label identification
            df["is_mal"] = (df["label_clean"] == "malicious").astype(int)

            # 3. Determine grouping (must match behavior_df columns)
            group_cols = ["id.orig_h", "time_bin"]
            if "capture_id" in df.columns:
                group_cols = ["capture_id", "id.orig_h", "time_bin"]

            # 4. Aggregate malicious statistics per window
            grp = (
                df.groupby(group_cols)["is_mal"]
                .agg(["sum", "count"])
                .reset_index()
            )

            # 5. Calculate malicious ratio (percentage of malicious flows in this window)
            grp["mal_ratio"] = grp["sum"] / grp["count"]

            # 6. Filtering Mixed Windows
            if drop_mixed:
                # Keep only purely Benign (0.0) or purely Malicious (1.0) windows
                grp = grp[(grp["mal_ratio"] == 0.0) | (grp["mal_ratio"] == 1.0)]

            # 7. Final Binary Label: 1 if window has malicious activity, else 0
            # (Even if not dropping mixed, any malicious presence = 1)
            grp["label"] = (grp["mal_ratio"] > 0).astype(int)

            # 8. Align labels with the behavior features
            # If drop_mixed is True, we use 'inner' to remove those windows from both X and y
            # If False, we use 'left' to keep everything and fill missing labels with 0
            labels_df = behavior_df.merge(
                grp[group_cols + ["label"]],
                on=group_cols,
                how="inner" if drop_mixed else "left"
            ).fillna(0)

            y = labels_df["label"].values

            return y, labels_df

import numpy as np
import pandas as pd
from sklearn.metrics import f1_score, precision_score, recall_score, classification_report, precision_recall_curve
from sklearn.preprocessing import RobustScaler
from sklearn.decomposition import PCA
from sklearn.ensemble import IsolationForest
from sklearn.pipeline import Pipeline
import warnings

warnings.filterwarnings("ignore")

# ==============================================================================
# THRESHOLD OPTIMIZATION UTILITIES
# ==============================================================================
def find_best_macro_threshold(y_true, scores):
    """Finds the threshold that maximizes the average F1 (Macro) of BOTH classes."""
    precisions, recalls, thresholds = precision_recall_curve(y_true, scores)

    # We sample thresholds to speed up the loop
    if len(thresholds) > 300:
        thresholds = np.random.choice(thresholds, 300, replace=False)

    best_f1 = 0
    best_t = 0

    for t in thresholds:
        y_pred = (scores >= t).astype(int)
        # Macro F1 is the average of Benign F1 and Attack F1
        f1 = f1_score(y_true, y_pred, average='macro')
        if f1 > best_f1:
            best_f1 = f1
            best_t = t

    return best_t, best_f1

# ==============================================================================
# CONFIGURATION
# ==============================================================================
# Reduced PCA slightly to stay safely below the ~18 features we generated
SEARCH_GRID = [
    {"ws": 60,  "roll": 3, "pca": 10},
    {"ws": 60,  "roll": 3, "pca": 6},
    {"ws": 30,  "roll": 3, "pca": 10},
    {"ws": 30,  "roll": 3, "pca": 6},
    {"ws": 120, "roll": 5, "pca": 6},
]

RANDOM_STATE = 42
best_overall_f1 = 0
best_cfg = None

# ==============================================================================
# EXECUTION LOOP
# ==============================================================================

capture_ids = full_train_df["capture_id"].unique()

from itertools import combinations

for k in [1, 2, 3]:  # 1 capture, then 2, then 3
    for combo in combinations(capture_ids, k):
        print(f"\nTraining with capture_ids = {combo}")

        train_subset = full_train_df[
            full_train_df["capture_id"].isin(combo)
        ]
        for cfg in SEARCH_GRID:
            print(f"\nðŸš€ Testing: Window={cfg['ws']}s, Smoothing={cfg['roll']}, PCA_Target={cfg['pca']}")

            # 1. Initialize Aggregator
            agg = ZeekFixedWindowAggregator(
                window_size=cfg['ws'],
                rolling_window=cfg['roll']
            )

            # 2. Transform Training Data (Benign Only)
            X_train_df, _ = agg.transform_with_metadata(train_subset)
            X_train_df = X_train_df.replace([np.inf, -np.inf], 0).fillna(0)

            # 3. Transform Testing Data & Generate Clean Labels
            # We use drop_mixed=True to remove ambiguous windows from the scoring
            X_test_raw_feat, test_meta = agg.transform_with_metadata(test_df)
            y_test, test_meta_clean = agg._generate_labels_internal(test_df, test_meta, drop_mixed=True)

            # 4. Align Features (Only keep rows that weren't dropped as 'mixed')
            # We also ensure column order matches between Train and Test
            common_cols = X_train_df.columns
            X_train = X_train_df[common_cols]

            # Filter X_test using the indices preserved in test_meta_clean
            X_test = X_test_raw_feat.loc[test_meta_clean.index]
            X_test = X_test[common_cols].replace([np.inf, -np.inf], 0).fillna(0)

            # 5. Dynamic PCA Safety Check
            # Prevents "n_components must be < n_features" error
            n_features = X_train.shape[1]
            actual_pca_c = min(cfg['pca'], n_features - 1)

            # 6. Define & Fit Pipeline
            model_pipeline = Pipeline([
                ('scaler', RobustScaler()),
                ('pca', PCA(n_components=actual_pca_c, random_state=RANDOM_STATE)),
                ('iso', IsolationForest(
                    n_estimators=100,
                    contamination='auto',
                    random_state=RANDOM_STATE,
                    n_jobs=-1
                ))
            ])

            model_pipeline.fit(X_train)

            # 7. Get Anomaly Scores (Negative decision function)
            test_scores = -model_pipeline.decision_function(X_test)

            # 8. Optimize Threshold for BALANCE (Macro F1)
            best_thresh, max_macro_f1 = find_best_macro_threshold(y_test, test_scores)

            # 9. Final Predictions & Detailed Metrics
            y_pred = (test_scores >= best_thresh).astype(int)
            attack_f1 = f1_score(y_test, y_pred) # F1 for class 1 only

            print(f"--- Results [PCA used: {actual_pca_c}] ---")
            print(classification_report(y_test, y_pred, target_names=["Benign", "Attack"], digits=4))
            print(f"Optimal Threshold: {best_thresh:.4f}")
            print(f"Macro-Average F1: {max_macro_f1:.4f}")

            # 10. Track the best model based on Macro F1 (Balance)
            if max_macro_f1 > best_overall_f1:
                best_overall_f1 = max_macro_f1
                best_cfg = {
                    **cfg,
                    "actual_pca": actual_pca_c,
                    "f1_macro": max_macro_f1,
                    "f1_attack": attack_f1,
                    "threshold": best_thresh
                }
                print("NEW BEST OVERALL CONFIGURATION FOUND!")

# ==============================================================================
# FINAL SUMMARY
# ==============================================================================
print("\n" + "="*40)
print("             FINAL BEST CONFIG             ")
print("="*40)
if best_cfg:
    for k, v in best_cfg.items():
        print(f"{k:15}: {v}")
print("="*40)

from sklearn.preprocessing import RobustScaler, MinMaxScaler

import tensorflow as tf
from tensorflow.keras import layers, models, callbacks
from tensorflow.keras.layers import LeakyReLU, GaussianNoise
from sklearn.preprocessing import RobustScaler, MinMaxScaler
import numpy as np

# ==============================================================================
# ADVANCED AUTOENCODER BUILDER
# ==============================================================================
def build_advanced_ae(input_dim, variant="baseline", arch=[12, 6]):
    """
    Builds different flavors of Autoencoders to break the performance plateau.
    """
    input_layer = layers.Input(shape=(input_dim,))

    # --- ENCODER ---
    if variant == "denoising":
        # Introduce noise to force the model to learn robust features
        x = GaussianNoise(0.05)(input_layer)
    else:
        x = input_layer

    for neurons in arch:
        x = layers.Dense(neurons)(x)
        # LeakyRelu prevents "Dead Neurons" which causes the plateau
        x = LeakyReLU(alpha=0.1)(x)

    # --- DECODER ---
    for neurons in reversed(arch[:-1]):
        x = layers.Dense(neurons)(x)
        x = LeakyReLU(alpha=0.1)(x)

    # Final layer: Linear activation is better for reconstruction when
    # using varied scaling methods.
    output_layer = layers.Dense(input_dim, activation='linear')(x)

    # Choose Loss Function
    # MAE is more robust to outliers; MSE is standard.
    loss_fn = 'mae' if variant == "mae_loss" else 'mse'

    model = models.Model(input_layer, output_layer)
    model.compile(optimizer='adam', loss=loss_fn)
    return model

# ==============================================================================
# CONFIGURATION & DATA PREP
# ==============================================================================
# Based on your previous best results
WS = 120
ROLL = 5

# We try 4 different "Logic" variants instead of just different layer counts
ADVANCED_GRID = [
    {"name": "MAE_Loss_Model", "variant": "mae_loss", "arch": [12, 6]},
    {"name": "Wide_Leaky",     "variant": "baseline", "arch": [32, 16, 8]},
    {"name": "Denoising_AE",   "variant": "denoising","arch": [12, 6]},
    {"name": "Deep_Leaky",     "variant": "baseline", "arch": [16, 12, 8, 4]}
]

print(f"Starting Advanced Autoencoder Evaluation (Window={WS}s, Roll={ROLL})")

# 1. Prepare Data
agg = ZeekFixedWindowAggregator(window_size=WS, rolling_window=ROLL)
X_train_raw, _ = agg.transform_with_metadata(full_train_df)
X_test_raw, test_meta = agg.transform_with_metadata(test_df)
y_test, test_meta_clean = agg._generate_labels_internal(test_df, test_meta, drop_mixed=True)

# 2. Preprocessing
# We use RobustScaler to handle network spikes, then MinMaxScaler for the NN
# This "Double Scaling" often breaks plateaus.
scaler_rob = RobustScaler()
scaler_min = MinMaxScaler()

X_train = scaler_rob.fit_transform(X_train_raw.replace([np.inf, -np.inf], 0).fillna(0))
X_train = scaler_min.fit_transform(X_train)

X_test_filtered = X_test_raw.loc[test_meta_clean.index].replace([np.inf, -np.inf], 0).fillna(0)
X_test = scaler_rob.transform(X_test_filtered)
X_test = scaler_min.transform(X_test)

input_dim = X_train.shape[1]

# ==============================================================================
# EXECUTION
# ==============================================================================
for cfg in ADVANCED_GRID:
    print(f"\n Testing Variant: {cfg['name']} | Architecture: {cfg['arch']}")

    model = build_advanced_ae(input_dim, variant=cfg['variant'], arch=cfg['arch'])

    early_stop = callbacks.EarlyStopping(
        monitor='val_loss',
        patience=8,
        restore_best_weights=True
    )

    model.fit(
        X_train, X_train,
        epochs=100, # Increased epochs because LeakyRelu learns more slowly
        batch_size=64,
        validation_split=0.15,
        verbose=0,
        callbacks=[early_stop]
    )

    # 3. Calculate Anomaly Score
    predictions = model.predict(X_test)
    # We use Mean Absolute Error for the score if the model used it for loss
    if cfg['variant'] == "mae_loss":
        scores = np.mean(np.abs(X_test - predictions), axis=1)
    else:
        scores = np.mean(np.power(X_test - predictions, 2), axis=1)

    # 4. Optimize Threshold
    best_thresh, max_macro_f1 = find_best_macro_threshold(y_test, scores)
    y_pred = (scores >= best_thresh).astype(int)

    print(f"--- Results: {cfg['name']} ---")
    print(classification_report(y_test, y_pred, target_names=["Benign", "Attack"], digits=4))
    print(f"Optimal Threshold: {best_thresh:.6f}")
    print(f"Macro-Average F1: {max_macro_f1:.4f}")

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from sklearn.metrics import classification_report


# 1. BEST PARAMETERS (From your results)
BEST_WS = 120
BEST_ROLL = 5
BEST_PCA = 6
BEST_THRESH = 0.08749525138718628
BEST_TRAIN_IDS = ('CTU-Honeypot-Capture-7-1',)

# 2. RE-TRAIN THE BEST MODEL
print(f" Re-training Best Model (WS={BEST_WS}, PCA={BEST_PCA})...")
agg = ZeekFixedWindowAggregator(window_size=BEST_WS, rolling_window=BEST_ROLL)

# Prepare Train Data
train_subset = full_train_df[full_train_df["capture_id"].isin(BEST_TRAIN_IDS)]
X_train_df, _ = agg.transform_with_metadata(train_subset)
feature_cols = X_train_df.columns

# Prepare Test Data
X_test_raw, test_meta = agg.transform_with_metadata(test_df)
y_true, test_meta_clean = agg._generate_labels_internal(test_df, test_meta, drop_mixed=True)

# 3. FIT PIPELINE
scaler = RobustScaler()
pca = PCA(n_components=BEST_PCA, random_state=42)
iso = IsolationForest(n_estimators=100, contamination='auto', random_state=42, n_jobs=-1)

X_train_vals = X_train_df.replace([np.inf, -np.inf], 0).fillna(0)
X_train_s = scaler.fit_transform(X_train_vals)
X_train_f = pca.fit_transform(X_train_s)
iso.fit(X_train_f)

# 4. PREDICT
X_test_vals = X_test_raw.loc[test_meta_clean.index][feature_cols].replace([np.inf, -np.inf], 0).fillna(0)
X_test_s = scaler.transform(X_test_vals)
X_test_f = pca.transform(X_test_s)
scores = -iso.decision_function(X_test_f)
y_pred = (scores >= BEST_THRESH).astype(int)

# 5. MAPPING BACK TO ORIGINAL FLOWS
print("Analyzing attack types...")

# Create a results dataframe with the grouping keys
# test_meta_clean contains the capture_id, id.orig_h, and time_bin
results_df = test_meta_clean.copy()
results_df['y_pred'] = y_pred
results_df['y_true'] = y_true

# Identify Missed Windows (True=1, Pred=0)
missed_windows = results_df[(results_df['y_true'] == 1) & (results_df['y_pred'] == 0)]

# We need to assign 'time_bin' to test_df to match them back
test_df_analyzed = test_df.copy()
test_df_analyzed['time_bin'] = (test_df_analyzed['ts'] // BEST_WS).astype(int)

# Group keys used by your aggregator
group_keys = ["capture_id", "id.orig_h", "time_bin"]

# Merge test_df with the missed_windows list to flag missed flows
missed_flows = test_df_analyzed.merge(
    missed_windows[group_keys],
    on=group_keys,
    how='inner'
)

# 6. CALCULATE ATTACK STATS
total_attacks = (
    test_df[test_df["label_clean"] == "malicious"]
    .groupby("malicious_type")
    .size()
    .rename("total")
)

missed_counts = (
    missed_flows[missed_flows["label_clean"] == "malicious"]
    .groupby("malicious_type")
    .size()
    .rename("missed")
)

attack_stats = pd.concat([total_attacks, missed_counts], axis=1).fillna(0)
attack_stats["detection_rate"] = (1 - attack_stats["missed"] / attack_stats["total"])
attack_stats = attack_stats.sort_values("detection_rate", ascending=False)

# 7. FINAL DISPLAY
print("\n" + "="*45)
print("             DETECTION STATISTICS             ")
print("="*45)
print(classification_report(y_true, y_pred, target_names=["benign", "malicious"], digits=4))
print("\n", attack_stats[['total', 'detection_rate']])

# 8. PLOT
plt.figure(figsize=(10, 6))
attack_stats["detection_rate"].plot(kind="barh", color="steelblue")
plt.xlabel("Detection Rate (Recall)")
plt.ylabel("Attack Type")
plt.title(f"Detection Accuracy per Attack Type\n(Isolation Forest | Window={BEST_WS}s)")
plt.xlim(0, 1.1)
plt.grid(axis="x", linestyle="--", alpha=0.5)

# Annotate percentages
for i, v in enumerate(attack_stats["detection_rate"]):
    plt.text(v + 0.01, i, f"{v*100:.1f}%", va='center', fontweight='bold')

plt.tight_layout()
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report

# ============================================================
# 1. APPLY TRAINED MODEL TO NEW DATA
# ============================================================
print("oading new capture data for monitoring...")
test = pd.read_parquet("/content/drive/MyDrive/HOMEGUARD/01_data/logs/02_notebooks/check.parquet")
test = test.drop_duplicates()

# Cleanup
test["ts"] = pd.to_numeric(test["ts"], errors="coerce")
test = test.dropna(subset=["ts"])

# Transform into windows
X_new_feat, behavior_df = agg.transform_with_metadata(test)
X_new_feat = X_new_feat.replace([np.inf, -np.inf], 0).fillna(0)

# Align columns and apply Pipeline (Scaler + PCA)
X_new = X_new_feat[feature_cols].values
X_new_s = scaler.transform(X_new)
X_new_f = pca.transform(X_new_s)

# Score and Predict
scores = -iso.decision_function(X_new_f)
y_pred = (scores >= BEST_THRESH).astype(int)

# Build results table
results = behavior_df[["id.orig_h", "time_bin"]].copy()
results["anomaly_score"] = scores
results["prediction"] = np.where(y_pred == 1, "Abnormal", "Normal")
results["window_start"] = results["time_bin"] * agg.window_size

# ============================================================
# 2. DEVICE-LEVEL TRIAGE (SUMMARY)
# ============================================================
device_summary = (
    results.groupby("id.orig_h")["prediction"]
    .value_counts()
    .unstack(fill_value=0)
)

# Ensure 'Abnormal' column exists even if none found
if "Abnormal" not in device_summary.columns: device_summary["Abnormal"] = 0

device_summary["abnormal_ratio"] = device_summary["Abnormal"] / (device_summary.sum(axis=1) + 1e-9)
device_summary = device_summary.sort_values("abnormal_ratio", ascending=False)

print("\nTOP SUSPICIOUS DEVICES:")
print(device_summary.head(10))

# ============================================================
# 3. TEMPORAL ANALYSIS (TIMELINE)
# ============================================================
# Select the most suspicious device for deep-dive
target_ip = device_summary.index[0]
ip_timeline = results[results["id.orig_h"] == target_ip].sort_values("window_start")

plt.figure(figsize=(15, 5))
plt.plot(ip_timeline["window_start"], ip_timeline["anomaly_score"], label="Anomaly Score", color='steelblue', alpha=0.7)
plt.axhline(y=BEST_THRESH, color='red', linestyle='--', label="Alert Threshold")

# Mark abnormal points
abnormal_pts = ip_timeline[ip_timeline["prediction"] == "Abnormal"]
plt.scatter(abnormal_pts["window_start"], abnormal_pts["anomaly_score"], color='red', s=40, label="Detection Event")

plt.title(f"Security Timeline: Activity for Device {target_ip}")
plt.xlabel("Time (Seconds)")
plt.ylabel("Anomaly Score")
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# ============================================================
# 4. AUTOMATED FORENSIC DRILL-DOWN (THE "WHY")
# ============================================================
print(f"\nExtracting Forensic Evidence for {target_ip}...")

# 1. Identify which time bins were flagged
bad_bins = ip_timeline[ip_timeline["prediction"] == "Abnormal"]["time_bin"].unique()

# 2. Extract raw flows from original data that match the bad IP and bad time bins
test['time_bin'] = (test['ts'] // agg.window_size).astype(int)
forensic_evidence = test[
    (test["id.orig_h"] == target_ip) &
    (test["time_bin"].isin(bad_bins))
]

if not forensic_evidence.empty:
    print(f"Found {len(forensic_evidence)} malicious flows associated with anomaly.")

    # Analyze Ports
    print("\n--- Target Ports ---")
    print(forensic_evidence["id.resp_p"].value_counts().head(5))

    # Analyze Destination IPs
    print("\n--- Top Targeted External Hosts ---")
    print(forensic_evidence["id.resp_h"].value_counts().head(5))

    # Analyze Connection States (S0 means connection attempt, common in scanning)
    if "conn_state" in forensic_evidence.columns:
        print("\n--- Connection States ---")
        print(forensic_evidence["conn_state"].value_counts())
else:
    print("No raw flows found for the abnormal window (Check window alignment).")

# ============================================================
# 5. FINAL VISUALIZATION: ABNORMAL RATIO PER DEVICE
# ============================================================
plt.figure(figsize=(12, 5))
sns.barplot(x=device_summary.index[:15], y=device_summary["Abnormal"][:15], palette="Reds_r")
plt.title("Volume of Abnormal Detections by Device (Top 15)")
plt.ylabel("Abnormal Windows Count")
plt.xlabel("Device IP Address")
plt.xticks(rotation=45)
plt.show()



# We are upgrading the Notebook to match the Orange Pi 3B exactly.
# Orange Pi Versions: Numpy 2.3.5 | Sklearn 1.7.2 | Pandas 2.3.3

import sys
import os

def install_matching_versions():
    print("Synchronizing environment with Orange Pi...")
    # We use 'pip install' with exact versions
    os.system('pip install numpy==2.3.5 scikit-learn==1.7.2 pandas==2.3.3 joblib')


install_matching_versions()

import sklearn
print(f"Current Sklearn Version: {sklearn.__version__}") # Should say 1.7.2

import joblib
import json
from google.colab import files

# 1. Save the trained models
joblib.dump(scaler, 'scaler.pkl')
joblib.dump(pca, 'pca.pkl')
joblib.dump(iso, 'iso_forest1.pkl')

# 2. Save the Threshold (using the best one found: 0.087495...)
threshold_data = {'threshold': 0.08749525138718628}
with open('threshold.json', 'w') as f:
    json.dump(threshold_data, f)

# 3. Save the Feature Column list (CRITICAL for alignment)
# This ensures the Orange Pi uses the columns in the exact same order
with open('feature_cols.json', 'w') as f:
    json.dump(list(feature_cols), f)

# 4. Download them all to your computer
file_list = ['scaler.pkl', 'pca.pkl', 'iso_forest1.pkl', 'threshold.json', 'feature_cols.json']

for file in file_list:
    files.download(file)

print("All 5 files are being downloaded to your 'Downloads' folder.")

import time
import pandas as pd
import numpy as np
import joblib
import os
import json
import subprocess
import collections
import psutil
from log_watcher import LogWatcher
from device_manager import DeviceManager

# --- CONFIGURATION ---
MODEL_DIR = os.path.expanduser("~/homeguard/models")
WINDOW_SIZE = 120
HISTORY_LEN = 5
LOG_FILE = os.path.expanduser("~/conn.log")
MAX_BUFFER_FLOWS = 50000

DATA_DIR = os.path.expanduser("~/homeguard/data")
TRAFFIC_HISTORY_FILE = os.path.join(DATA_DIR, "traffic_history.json")
ALERTS_FILE = os.path.join(DATA_DIR, "alerts.json")
DEVICES_FILE = os.path.join(DATA_DIR, "devices.json")
SYSTEM_STATS_FILE = os.path.join(DATA_DIR, "system_stats.json")

os.makedirs(DATA_DIR, exist_ok=True)

# ==============================================================================
# UPDATED AGGREGATOR
# ==============================================================================
class ZeekFixedWindowAggregator:
    def __init__(self, window_size=120, rolling_window=5):
        self.window_size = window_size
        self.rolling_window = rolling_window

    def _assign_fixed_bins(self, df):
        df = df.copy()
        df["time_bin"] = (df["ts"] // self.window_size).astype(int)
        return df

    def transform_with_metadata(self, df):
        df = df.copy()
        num_cols = ["ts", "orig_bytes", "resp_bytes", "orig_pkts", "resp_pkts", "duration", "missed_bytes"]
        for c in num_cols:
            if c in df.columns:
                df[c] = pd.to_numeric(df[c], errors="coerce").fillna(0)

        df = self._assign_fixed_bins(df)
        group_cols = ["id.orig_h", "time_bin"]

        agg = df.groupby(group_cols).agg(
            orig_bytes_sum=("orig_bytes", "sum"),
            orig_bytes_mean=("orig_bytes", "mean"),
            orig_bytes_std=("orig_bytes", "std"),
            resp_bytes_sum=("resp_bytes", "sum"),
            resp_bytes_mean=("resp_bytes", "mean"),
            orig_pkts_sum=("orig_pkts", "sum"),
            orig_pkts_mean=("orig_pkts", "mean"),
            resp_pkts_sum=("resp_pkts", "sum"),
            resp_pkts_mean=("resp_pkts", "mean"),
            duration_mean=("duration", "mean"),
            duration_std=("duration", "std"),
            flow_count=("ts", "count"),
            orig_port_diversity=("id.orig_p", "nunique"),
            resp_port_diversity=("id.resp_p", "nunique"),
            missed_bytes_sum=("missed_bytes", "sum"),
            tcp_ratio=("proto", lambda x: (x == "tcp").mean()),
            s0_ratio=("conn_state", lambda x: (x == "S0").mean())
        ).reset_index().fillna(0)

        agg["bytes_ratio"] = agg["orig_bytes_sum"] / (agg["resp_bytes_sum"] + 1)
        agg["pkts_per_sec"] = (agg["orig_pkts_sum"] + agg["resp_pkts_sum"]) / self.window_size
        agg["duration_cv"] = agg["duration_std"] / (agg["duration_mean"] + 1)

        log_cols = ["orig_bytes_sum", "orig_bytes_mean", "resp_bytes_sum", "orig_pkts_sum", "resp_pkts_sum", "flow_count"]
        for c in log_cols:
            if c in agg.columns:
                agg[c] = np.log1p(agg[c])

        if self.rolling_window > 1:
            agg = agg.sort_values(["id.orig_h", "time_bin"])
            feat_cols = agg.select_dtypes(include=[np.number]).columns.difference(["time_bin"])
            agg[feat_cols] = agg.groupby("id.orig_h")[feat_cols].transform(
                lambda x: x.rolling(window=self.rolling_window, min_periods=1).mean()
            )

        out = agg.fillna(0)
        X_feat = out.drop(columns=["time_bin", "id.orig_h"], errors="ignore")
        return X_feat, out

# ==============================================================================
# HOMEGUARD BRAIN
# ==============================================================================
class HomeGuardBrain:
    def __init__(self):
        print(f"Initializing HomeGuard AI (Window={WINDOW_SIZE}s, Roll={HISTORY_LEN})...")
        try:
            self.scaler = joblib.load(os.path.join(MODEL_DIR, "scaler.pkl"))
            self.iso = joblib.load(os.path.join(MODEL_DIR, "iso_forest1.pkl"))
            self.pca = joblib.load(os.path.join(MODEL_DIR, "pca.pkl"))
            with open(os.path.join(MODEL_DIR, "threshold.json")) as f:
                self.threshold = json.load(f)["threshold"]
            with open(os.path.join(MODEL_DIR, "feature_cols.json")) as f:
                self.feature_cols = json.load(f)
            print("Models & Thresholds Loaded.")
        except Exception as e:
            print(f"CRITICAL LOAD ERROR: {e}")
            exit(1)

        self.traffic_buffer = []
        self.start_time = time.time()
        self.device_manager = DeviceManager()
        self.device_manager.refresh()
        self.aggregator = ZeekFixedWindowAggregator(window_size=WINDOW_SIZE, rolling_window=HISTORY_LEN)

    # --- RESTORED UTILITIES ---
    def _get_human_time(self):
        return time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())

    def _append_json_log(self, path, entry):
        with open(path, "a") as f:
            f.write(json.dumps(entry) + "\n")

    def _load_json(self, path, default):
        if not os.path.exists(path): return default
        try:
            with open(path, "r") as f: return json.load(f)
        except: return default

    def _save_json(self, path, data):
        with open(path, "w") as f: json.dump(data, f, indent=2)

    def save_health_stats(self, duration, flow_count, device_count, anomaly_count):
        stats = {
            "timestamp": self._get_human_time(),
            "latency_seconds": round(duration, 4),
            "flows_processed": flow_count,
            "devices_active": device_count,
            "anomalies_found": anomaly_count,
            "cpu_usage_percent": psutil.cpu_percent(),
            "ram_usage_mb": round(psutil.Process().memory_info().rss / 1024 / 1024, 2)
        }
        self._append_json_log(SYSTEM_STATS_FILE, stats)
        print(f" [STATS] Latency: {stats['latency_seconds']}s | RAM: {stats['ram_usage_mb']}MB | Anomalies: {anomaly_count}/{device_count}")

    def save_traffic_window(self, device_ip, features, anomaly_score, prediction, alert_id=None):
        entry = {
            "timestamp": self._get_human_time(),
            "device_ip": device_ip,
            "device_name": self.device_manager.get_device_name(device_ip),
            "features": features,
            "anomaly_score": float(anomaly_score),
            "prediction": prediction,
            "alert_id": alert_id
        }
        self._append_json_log(TRAFFIC_HISTORY_FILE, entry)

    def update_device_status(self, ip, status, blocked=False, alert_id=None):
        devices = self._load_json(DEVICES_FILE, {})
        if ip not in devices:
            devices[ip] = {"ip": ip, "name": self.device_manager.get_device_name(ip), "first_seen": self._get_human_time()}
        devices[ip]["last_seen"] = self._get_human_time()
        devices[ip]["status"] = status
        if blocked: devices[ip]["blocked"] = True
        if alert_id: devices[ip]["last_alert_id"] = alert_id
        self._save_json(DEVICES_FILE, devices)

    def is_blocked(self, ip):
        devices = self._load_json(DEVICES_FILE, {})
        return devices.get(ip, {}).get("blocked", False)

    def trigger_alert(self, ip, anomaly_score):
        device_name = self.device_manager.get_device_name(ip)
        alert_id = f"ALRT-{int(time.time())}"
        print(f"ALERT: Anomaly on {ip} ({device_name})! Score: {anomaly_score:.4f}")
        try:
            subprocess.run(["/home/orangepi/scripts/block_ip.sh", ip], timeout=5)
        except Exception as e:
            print(f"Firewall Error: {e}")

        alert = {"alert_id": alert_id, "timestamp": self._get_human_time(), "device": {"ip": ip, "name": device_name}, "anomaly_score": float(anomaly_score), "severity": "High", "reason": "Anomalous traffic behavior", "action": "Blocked"}
        self._append_json_log(ALERTS_FILE, alert)
        self.update_device_status(ip, "Blocked", blocked=True, alert_id=alert_id)
        return alert_id

    def parse_line(self, line):
        if line.startswith("#"): return None
        parts = line.split('\t')
        if len(parts) < 20: return None
        def safe_f(x): return float(x) if x != '-' else 0.0
        return {'ts': float(parts[0]), 'id.orig_h': parts[2], 'id.orig_p': parts[3], 'id.resp_h': parts[4], 'id.resp_p': parts[5], 'proto': parts[6], 'duration': safe_f(parts[8]), 'orig_bytes': safe_f(parts[9]), 'resp_bytes': safe_f(parts[10]), 'conn_state': parts[11], 'missed_bytes': safe_f(parts[14]), 'orig_pkts': safe_f(parts[16]), 'resp_pkts': safe_f(parts[18])}

    def process_window(self):
        if not self.traffic_buffer: return
        t_start = time.time()
        full_df = pd.DataFrame(self.traffic_buffer)
        print(f" [PROCESSING] {len(full_df)} flows...")

        X_feat, behavior_df = self.aggregator.transform_with_metadata(full_df)
        X_feat_aligned = X_feat.reindex(columns=self.feature_cols, fill_value=0)

        X_scaled = self.scaler.transform(X_feat_aligned)
        X_pca    = self.pca.transform(X_scaled)
        scores   = -self.iso.decision_function(X_pca)
        preds    = scores >= self.threshold

        total_devices = 0
        total_anomalies = 0

        for i, row in behavior_df.iterrows():
            device_ip = row["id.orig_h"]
            if self.is_blocked(device_ip): continue

            total_devices += 1
            anomaly_score = scores[i]
            is_anomaly = preds[i]

            alert_id = None
            prediction = "normal"

            if is_anomaly:
                total_anomalies += 1
                alert_id = self.trigger_alert(device_ip, anomaly_score)
                prediction = "malicious"
            else:
                self.update_device_status(device_ip, "Online")

            current_features = row.drop(["id.orig_h", "time_bin"], errors="ignore").to_dict()
            current_features = {k: float(v) if isinstance(v, (np.float64, np.int64, np.float32)) else v for k, v in current_features.items()}

            self.save_traffic_window(device_ip, current_features, anomaly_score, prediction, alert_id)

        duration = time.time() - t_start
        self.save_health_stats(duration, len(self.traffic_buffer), total_devices, total_anomalies)

    def run(self):
        watcher = LogWatcher(LOG_FILE)
        print(f"HomeGuard Core Active. Monitoring {LOG_FILE}...")
        for line in watcher.monitor():
            data = self.parse_line(line)
            if data: self.traffic_buffer.append(data)
            if len(self.traffic_buffer) > MAX_BUFFER_FLOWS or (time.time() - self.start_time >= WINDOW_SIZE):
                try:
                    self.process_window()
                except Exception as e:
                    print(f"Process Error: {e}")
                self.traffic_buffer = []
                self.start_time = time.time()

if __name__ == "__main__":
    HomeGuardBrain().run()

"""# base line"""

class ZeekFeatureEngineer(BaseEstimator, TransformerMixin):
    def __init__(self):
        pass

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        df = X.copy()

        # 1. Imputation based on Research Findings
        # 'service' NaN means 'unknown protocol' = This is a signal.
        cat_cols = ['service', 'history', 'conn_state', 'proto']
        for col in cat_cols:
            if col in df.columns:
                df[col] = df[col].fillna('unknown')

        # Physics: Missing duration/bytes usually means failed connection = 0
        num_cols = ['duration', 'orig_bytes', 'resp_bytes', 'missed_bytes', 'orig_pkts', 'orig_ip_bytes', 'resp_pkts', 'resp_ip_bytes']
        for col in num_cols:
            if col in df.columns:
                df[col] = df[col].fillna(0)

        # 2. Feature Construction (Behavioral Layers
        # Log Transform (Power Law distribution fix)
        for col in num_cols:
            if col in df.columns:
                df[col] = np.log1p(df[col])

        # Ratio: Bytes per Packet (Payload Density)
        if 'orig_bytes' in df.columns and 'orig_pkts' in df.columns:
            # Adding epsilon 1e-6 to avoid division by zero if not using log subtraction logic
            # Since we logged them, we subtract: log(A/B) = log(A) - log(B)
            df['orig_bytes_per_pkt'] = df['orig_bytes'] - df['orig_pkts']

        # Ratio: Response Symmetry
        if 'resp_bytes' in df.columns and 'orig_bytes' in df.columns:
             df['resp_orig_ratio'] = df['resp_bytes'] - df['orig_bytes']

        # 3. Forensics: System Ports
        # Check if port is System (0-1023) or User (1024+)
        # We cast to int first just in case
        if 'id.resp_p' in df.columns:
             df['is_system_port'] = (df['id.resp_p'].astype(float) < 1024).astype(int)

        return df

# --- PIPELINE CONFIGURATION ---

# Define columns
numeric_features = [
    'duration', 'orig_bytes', 'resp_bytes', 'missed_bytes',
    'orig_pkts', 'orig_ip_bytes', 'resp_pkts', 'resp_ip_bytes',
    'orig_bytes_per_pkt', 'resp_orig_ratio', 'is_system_port'
]

# We treat Ports as Categorical (Destinations), but only top ones
categorical_features = ['proto', 'service', 'conn_state', 'history', 'id.resp_p']

# 1. The Column Transformer
preprocessor = ColumnTransformer(
    transformers=[
        # Scale numbers
        ('num', StandardScaler(), numeric_features),

        # Encode categories.
        # max_categories=20 is CRITICAL. It groups rare ports/histories into "infrequent_sklearn".
        # handle_unknown='ignore' ensures the pipeline doesn't crash on new malware ports.
        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=True, max_categories=20), categorical_features)
    ],
    remainder='drop', # Drop IDs, timestamps, labels
    verbose_feature_names_out=False
)

# 2. The Master Pipeline
pipeline = Pipeline([
    ('engineering', ZeekFeatureEngineer()),
    ('preprocessing', preprocessor)
])


# Fit on TRAIN only (Benign Baseline)
X_train_processed = pipeline.fit_transform(train_df)

# Transform Valid and Test
# X_valid_processed = pipeline.transform(valid_df)
X_test_processed = pipeline.transform(test_df)

# Extract Feature Names for verification
feature_names = pipeline.named_steps['preprocessing'].get_feature_names_out()

print(f"Pipeline Fitted Successfully.")
print(f"Input Features: {len(train_df.columns)}")
print(f"Output Features: {len(feature_names)}")
print(f"Feature Matrix Shape: {X_train_processed.shape}")

import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.decomposition import TruncatedSVD
from sklearn.preprocessing import RobustScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, f1_score, precision_recall_curve

RANDOM_STATE = 42

def find_best_macro_threshold(y_true, scores):
    precisions, recalls, thresholds = precision_recall_curve(y_true, scores)
    if len(thresholds) > 300:
        thresholds = np.random.choice(thresholds, 300, replace=False)

    best_f1, best_t = 0, 0
    for t in thresholds:
        y_pred = (scores >= t).astype(int)
        f1 = f1_score(y_true, y_pred, average="macro")
        if f1 > best_f1:
            best_f1, best_t = f1, t
    return best_t, best_f1

# ------------------------------------------------------------
# Data
# ------------------------------------------------------------
X_train_df = train_df[train_df["label_clean"] == "benign"].copy()
X_test_df  = test_df.copy()
y_test = (X_test_df["label_clean"] == "malicious").astype(int).values

# ------------------------------------------------------------
# Feature pipeline (yours)
# ------------------------------------------------------------
X_train = pipeline.fit_transform(X_train_df)  # sparse
X_test  = pipeline.transform(X_test_df)       # sparse

# ------------------------------------------------------------
# Fine tuning
# ------------------------------------------------------------
SEARCH_GRID = [
    {"trees": 200, "cont": "auto", "svd": 25},
    {"trees": 300, "cont": "auto", "svd": 10},
    {"trees": 400, "cont": 0.03,   "svd": 25},
    {"trees": 400, "cont": 0.05,   "svd": 10},
]

best_overall_f1, best_cfg = 0, None

for cfg in SEARCH_GRID:
    print("\nTesting Config:", cfg)

    # RobustScaler can work with sparse if we disable centering
    scaler = RobustScaler(with_centering=False)

    # SVD works with sparse (PCA replacement)
    svd = TruncatedSVD(n_components=cfg["svd"], random_state=RANDOM_STATE)

    model = IsolationForest(
        n_estimators=cfg["trees"],
        contamination=cfg["cont"],
        random_state=RANDOM_STATE,
        n_jobs=-1
    )

    model_pipeline = Pipeline([
        ("scaler", scaler),
        ("svd", svd),
        ("iso", model)
    ])

    model_pipeline.fit(X_train)

    # IMPORTANT: score the TEST set
    test_scores = -model_pipeline.decision_function(X_test)

    best_t, macro_f1 = find_best_macro_threshold(y_test, test_scores)
    y_pred = (test_scores >= best_t).astype(int)
    attack_f1 = f1_score(y_test, y_pred)

    print("Macro F1 :", round(macro_f1, 4))
    print("Attack F1:", round(attack_f1, 4))
    print("Threshold:", round(best_t, 6))
    print(classification_report(y_test, y_pred, target_names=["Benign", "Attack"], digits=4))

    if macro_f1 > best_overall_f1:
        best_overall_f1 = macro_f1
        best_cfg = {**cfg, "macro_f1": macro_f1, "attack_f1": attack_f1, "threshold": best_t}
        print("NEW BEST CONFIG FOUND!")

print("\nBEST CONFIG:", best_cfg)

# ============================================================
# AUTOENCODER ANOMALY DETECTION (FLOW-LEVEL, NEW PIPELINE)
# ============================================================

import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models, callbacks
from tensorflow.keras.layers import LeakyReLU, GaussianNoise
from sklearn.preprocessing import RobustScaler, MinMaxScaler
from sklearn.metrics import classification_report, f1_score, precision_recall_curve
import warnings

warnings.filterwarnings("ignore")
tf.random.set_seed(42)
np.random.seed(42)

# ------------------------------------------------------------
# Threshold Optimization (Macro F1)
# ------------------------------------------------------------
def find_best_macro_threshold(y_true, scores):
    precision, recall, thresholds = precision_recall_curve(y_true, scores)

    if len(thresholds) > 300:
        thresholds = np.random.choice(thresholds, 300, replace=False)

    best_f1, best_t = 0, 0
    for t in thresholds:
        y_pred = (scores >= t).astype(int)
        f1 = f1_score(y_true, y_pred, average="macro")
        if f1 > best_f1:
            best_f1, best_t = f1, t

    return best_t, best_f1


# ------------------------------------------------------------
# Advanced Autoencoder Builder (UNCHANGED LOGIC)
# ------------------------------------------------------------
def build_advanced_ae(input_dim, variant="baseline", arch=[12, 6]):
    inp = layers.Input(shape=(input_dim,))

    x = GaussianNoise(0.05)(inp) if variant == "denoising" else inp

    for n in arch:
        x = layers.Dense(n)(x)
        x = LeakyReLU(alpha=0.1)(x)

    for n in reversed(arch[:-1]):
        x = layers.Dense(n)(x)
        x = LeakyReLU(alpha=0.1)(x)

    out = layers.Dense(input_dim, activation="linear")(x)

    loss_fn = "mae" if variant == "mae_loss" else "mse"

    model = models.Model(inp, out)
    model.compile(optimizer="adam", loss=loss_fn)
    return model


# ------------------------------------------------------------
# Model Variants (Same Logic as Before)
# ------------------------------------------------------------
ADVANCED_GRID = [
    {"name": "MAE_Loss_Model", "variant": "mae_loss", "arch": [12, 6]},
    {"name": "Wide_Leaky",     "variant": "baseline", "arch": [32, 16, 8]},
    {"name": "Denoising_AE",   "variant": "denoising","arch": [12, 6]},
    {"name": "Deep_Leaky",     "variant": "baseline", "arch": [16, 12, 8, 4]},
]


# ------------------------------------------------------------
# 1. DATA PREP (PIPELINE-BASED)
# ------------------------------------------------------------
X_train_df = train_df[train_df["label_clean"] == "benign"].copy()
X_test_df  = test_df.copy()

y_test = (X_test_df["label_clean"] == "malicious").astype(int).values

# Feature engineering + encoding
X_train_feat = pipeline.fit_transform(X_train_df)
X_test_feat  = pipeline.transform(X_test_df)

# FORCE DENSE (required for NN)
if not isinstance(X_train_feat, np.ndarray):
    X_train_feat = X_train_feat.toarray()
    X_test_feat  = X_test_feat.toarray()

# Double scaling (dense-safe)
scaler_rob = RobustScaler()
scaler_min = MinMaxScaler()

X_train_scaled = scaler_rob.fit_transform(X_train_feat)
X_train_scaled = scaler_min.fit_transform(X_train_scaled)

X_test_scaled = scaler_rob.transform(X_test_feat)
X_test_scaled = scaler_min.transform(X_test_scaled)

input_dim = X_train_scaled.shape[1]

# ------------------------------------------------------------
# 2. DOUBLE SCALING (CRITICAL)
# ------------------------------------------------------------
scaler_rob = RobustScaler()
scaler_min = MinMaxScaler()

X_train_scaled = scaler_rob.fit_transform(X_train_feat)
X_train_scaled = scaler_min.fit_transform(X_train_scaled)

X_test_scaled = scaler_rob.transform(X_test_feat)
X_test_scaled = scaler_min.transform(X_test_scaled)

input_dim = X_train_scaled.shape[1]


# ------------------------------------------------------------
# 3. EXECUTION LOOP
# ------------------------------------------------------------
best_overall_f1 = 0
best_cfg = None

for cfg in ADVANCED_GRID:
    print(f"\nðŸš€ Testing {cfg['name']} | Arch={cfg['arch']}")

    model = build_advanced_ae(
        input_dim=input_dim,
        variant=cfg["variant"],
        arch=cfg["arch"]
    )

    early_stop = callbacks.EarlyStopping(
        monitor="val_loss",
        patience=8,
        restore_best_weights=True
    )

    model.fit(
        X_train_scaled, X_train_scaled,
        epochs=100,
        batch_size=64,
        validation_split=0.15,
        verbose=0,
        callbacks=[early_stop]
    )

    # --------------------------------------------------------
    # Scoring
    # --------------------------------------------------------
    recon = model.predict(X_test_scaled, verbose=0)

    if cfg["variant"] == "mae_loss":
        scores = np.mean(np.abs(X_test_scaled - recon), axis=1)
    else:
        scores = np.mean(np.square(X_test_scaled - recon), axis=1)

    # Threshold tuning
    best_t, macro_f1 = find_best_macro_threshold(y_test, scores)
    y_pred = (scores >= best_t).astype(int)
    attack_f1 = f1_score(y_test, y_pred)

    print("Macro F1 :", round(macro_f1, 4))
    print("Attack F1:", round(attack_f1, 4))
    print("Threshold:", round(best_t, 6))
    print(classification_report(y_test, y_pred, target_names=["Benign", "Attack"], digits=4))

    if macro_f1 > best_overall_f1:
        best_overall_f1 = macro_f1
        best_cfg = {
            **cfg,
            "macro_f1": macro_f1,
            "attack_f1": attack_f1,
            "threshold": best_t
        }
        print("NEW BEST MODEL")


# ------------------------------------------------------------
# FINAL SUMMARY
# ------------------------------------------------------------
print("\n" + "="*45)
print("        BEST AUTOENCODER CONFIG")
print("="*45)
for k, v in best_cfg.items():
    print(f"{k:15}: {v}")
print("="*45)